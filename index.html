<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Practical Machine Learning Prediction Assignment Writeup by michaelmalinjr</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Practical Machine Learning Prediction Assignment Writeup</h1>
        <p></p>

        <p class="view"><a href="https://github.com/michaelmalinjr/datasciencecoursera">View the Project on GitHub <small>michaelmalinjr/datasciencecoursera</small></a></p>


        <ul>
          <li><a href="https://github.com/michaelmalinjr/datasciencecoursera/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/michaelmalinjr/datasciencecoursera/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/michaelmalinjr/datasciencecoursera">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <p>title: "Practical Machine Learning Prediction Assignment Writeup"</p>

<p>author: "Michael Malin Jr"</p>

<p>date: "Friday, June 19, 2015"</p>

<p>Brief Description:</p>

<p>With wearable devices, it is now possible to collect large amounts of data to quantify particular exercises. Not much research has been done to show how correctly someone does a particular exercise. The goal of the Practical Machine Learning project is to predict if someone is doing the exercise correctly or incorrectly. Data was taken from the accelerometers on the belt, forearm, arm, and dumbell from 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. </p>

<p>More information is available from the website here: <a href="http://groupware.les.inf.puc-rio.br/har">http://groupware.les.inf.puc-rio.br/har</a> (see the section on the Weight Lifting Exercise Dataset).</p>

<p>For more information on the Weight Lifting Exercises Dataset, please refer to the researh paper at <a href="http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201">http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201</a>. The authors of the paper are cited below: </p>

<p>Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.</p>

<p>Analysis:</p>

<p>The training data for this project are available here: </p>

<p><a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv</a></p>

<p>The test data are available here: </p>

<p><a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv</a></p>

<p>First load the training and test data into R with read.csv("", header=T). Use View() to view your dataset into R. The "V" in View() is capitalized. </p>

<p>Second, with such a large data set you would want to clean your data to remove missing data and unnecessary columns that might decrease the accuracy of your prediction model. I choose to remove the any columns associated with the timestamps, subject names, window numbers, and if a new window or not.
The reason why you would want to remove subject names is to avoid producing a model for each individual subject. If you keep the window numbers then the classifier can use it to pick the outcome of the model. If you keep the timestamps in your model then the classifier will use vectors with nearby times. I used subset(, select= c(:)) to filter my data.  </p>

<p>Third, you would want to measure the importance of each column in your model. Choose the most important columns to fit your model with to improve the accuracy of your model and to prevent overfitting your model. The prediction models are in the caret package. Download the caret package with, install.packages("caret"), and load the caret library with, library(caret). Train your model on the "classe" variable with, train(classe~ ., data= , method="rf", importance=TRUE).
Random forest (method="rf") uses cross validation when you run your model to split your data.</p>

<p>Warning: Model might take hours to run depending on your computer. After running your model, you might want to save it to your computer with, saveRDS(, "name.Rds"). Saving your file will help prevent you from having to run your model every time you shutdown and restart R. Use, readRDS("name.Rds") to load your model when you restart R.</p>

<p>Next, use varImp() to list the variables by maximum importance across the classes. I choose to use the top 10 variables for my model. Take the list of most important variables and filter your training data set with subset(). </p>

<p>Fourth, predict on your model with predict(, ). View the accuracy of your model with, confusionMatrix(, $classe).
Below, is my model with 100% Accuracy and Kappa. 100% Kappa means that the classifier is in total agreement with a random classifier.</p>

<p>Confusion Matrix and Statistics</p>

<p>Overall Statistics</p>

<p>Accuracy : 1<br>
95% CI : (0.9998, 1)
No Information Rate : 0.2844<br>
P-Value [Acc &gt; NIR] : &lt; 2.2e-16  </p>

<p>Kappa : 1<br>
Mcnemar''s Test P-Value : NA         </p>

<p>Use print($finalModel) to view the out of bag (OOB) error rate. </p>

<p>Call:
randomForest(x = x, y = y, mtry = param$mtry, importance = TRUE)
Type of random forest: classification
Number of trees: 500
No. of variables tried at each split: 2</p>

<p>OOB estimate of  error rate: 4.35%</p>

<p>Fifth, Clean your testing data similar to how you cleaned your training data.
Run your prediction algorithm on your filtered testing data with, predict(, ). Compare the OOB error rate when running your prediction model on your testing data.</p>

<p>My first prediction model had an OOB error rate of 14%. On my first attempt I made the mistake by filtering on too much data. I went back and refit my model and on the variables with maximum importance across the classes. I filtered on my new variables and retrained my model.</p>

<p>My second prediction model had an OOB error rate of 4% and I was able to predict on 19/20 test cases correctly.  </p>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/michaelmalinjr">michaelmalinjr</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>
